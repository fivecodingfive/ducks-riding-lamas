_wandb:
    value:
        cli_version: 0.19.11
        code_path: code/main_ppo.py
        m:
            - "1": entropy_coef
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode
              "6":
                - 3
              "7": []
            - "1": policy_loss
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": reward
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": value_loss
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": entropy_bonus
              "5": 2
              "6":
                - 1
                - 3
              "7": []
            - "1": episode_length
              "5": 2
              "6":
                - 1
                - 3
              "7": []
        python_version: 3.11.9
        t:
            "1":
                - 2
                - 3
                - 55
            "2":
                - 2
                - 3
                - 55
            "3":
                - 7
                - 13
                - 15
                - 16
                - 17
                - 23
                - 55
                - 61
                - 62
            "4": 3.11.9
            "5": 0.19.11
            "8":
                - 3
                - 5
            "10":
                - 3
            "12": 0.19.11
            "13": windows-amd64
environment:
    value:
        data_dir: ./data
        mode: training
        variant: 0
model:
    value:
        clip_ratio: 0.2
        entropy_decay: 0.98
        entropy_min: 0
        entropy_start: 0.1
training:
    value:
        device: cpu
        episodes: 20
        gamma: 0.99
        lam: 0.9
        max_time_steps: 200
        policy_epochs: 40
        policy_lr: 0.0009
        seed: 42
        value_epochs: 40
        value_lr: 0.002
